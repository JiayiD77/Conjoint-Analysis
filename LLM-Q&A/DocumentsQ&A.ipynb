{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documents Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load your documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(file):\n",
    "    # Using URL for file param can load data from web.\n",
    "    name, extension = os.path.splitext(file)\n",
    "\n",
    "    if extension == '.pdf':\n",
    "        from langchain.document_loaders import PyPDFLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = PyPDFLoader(file)\n",
    "    elif extension == '.docx':\n",
    "        from langchain.document_loaders import Docx2txtLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = Docx2txtLoader(file)\n",
    "    else:\n",
    "        print('File type not supported!')\n",
    "\n",
    "    data = loader.load()\n",
    "    return data\n",
    "\n",
    "def load_from_wiki(query, lang='en', load_max_docs=1):\n",
    "    from langchain.document_loaders import WikipediaLoader\n",
    "    loader = WikipediaLoader(query=query, lang=lang, load_max_docs=load_max_docs)\n",
    "    data = loader.load\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(data, chunk_size):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding and Uploading to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_or_fetch_embeddings(index_name, chunks=None):\n",
    "    import pinecone\n",
    "    from pinecone import ServerlessSpec\n",
    "    from langchain.vectorstores import Pinecone\n",
    "    from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    pc = pinecone.Pinecone(api_key = os.environ.get('PINECONE_API_KEY'))\n",
    "\n",
    "    indexes = pc.list_indexes()\n",
    "    index_exist = False\n",
    "    for index in indexes:\n",
    "        if index['name'] == index_name:\n",
    "            index_exist = True\n",
    "            vector_store = Pinecone.from_existing_index(index_name, embeddings)\n",
    "    \n",
    "    if not index_exist:\n",
    "        pc.create_index(index_name, dimension=1536, metric='cosine', spec=ServerlessSpec(cloud=\"aws\", region=\"us-west-2\"))\n",
    "        vector_store = Pinecone.from_documents(chunks, embeddings, index_name=index_name)\n",
    "    \n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pinecone_index(index_name='all'):\n",
    "    import pinecone\n",
    "    pc = pinecone.Pinecone(api_key = os.environ.get('PINECONE_API_KEY'))\n",
    "    \n",
    "    if index_name == 'all':\n",
    "        indexes = pc.list_indexes()\n",
    "        for index in indexes:\n",
    "            pc.delete_index(index['name'])\n",
    "    else:\n",
    "        pc.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asking and Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(vector_store, question):\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "    llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=1)\n",
    "\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k':3})\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type='stuff', retriever=retriever)\n",
    "\n",
    "    answer = chain.run(question)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_with_memory(vector_store, q, chat_history=[]):\n",
    "    from langchain.chains import ConversationalRetrievalChain\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "    llm = ChatOpenAI(temperature=1)\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k':3})\n",
    "\n",
    "    crc = ConversationalRetrievalChain.from_llm(llm, retriever)\n",
    "    result = crc({'question':q, 'chat_history':chat_history})\n",
    "    chat_history.append((q, result['answer']))\n",
    "\n",
    "    return result, chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading https://arxiv.org/pdf/1706.03762.pdf\n"
     ]
    }
   ],
   "source": [
    "data = load_document('https://arxiv.org/pdf/1706.03762.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to check the content info\n",
    "# print(f'There is a total of {len(data)} pages.')\n",
    "# print(data[0].page_content)\n",
    "# print(data[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_data(data, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunks[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all the indexes\n",
    "# delete_pinecone_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "index_name = 'attention'\n",
    "vector_store = insert_or_fetch_embeddings(index_name=index_name, chunks=chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Transformer algorithm is implemented using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. The architecture of the Transformer model is shown in Figure 1, with the encoder on the left and the decoder on the right. The model relies entirely on self-attention to compute representations of its input and output without using sequence-based recurrence or convolution. Additionally, the model allows for parallelization and can achieve state-of-the-art translation quality after being trained for only twelve hours on eight P100 GPUs.\n"
     ]
    }
   ],
   "source": [
    "question = 'How is the Transformer algorithm impelmented?'\n",
    "answer = get_answer(vector_store, question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type Exit to quit chat.\n",
      "Question 1: exir\n",
      "\n",
      "\n",
      "Answer: I'm sorry, but I'm not sure what you mean by \"exir.\" Can you please provide more context or clarify your question?\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Quitting...\n",
      "Byebye!\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "i = 1\n",
    "print('Type Exit to quit chat.')\n",
    "while True:\n",
    "    q = input(f'Please enter you question: ')\n",
    "    \n",
    "    \n",
    "    if q.lower() == 'exit':\n",
    "        print('Quitting...\\nByebye!')\n",
    "        time.sleep(2)\n",
    "        break\n",
    "    \n",
    "    print(f'Question {i}: {q}\\n')\n",
    "    answer = get_answer(vector_store, q)\n",
    "    print(f'\\nAnswer: {answer}')\n",
    "    print(f'\\n{\"-\"* 50}\\n')\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q&A with history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Transformer is different from other neural networks in that it does not rely on recurrence (such as in recurrent neural networks) or convolution (such as in convolutional neural networks) for capturing dependencies between input and output. Instead, it uses an attention mechanism that allows it to draw global dependencies between input and output. The use of self-attention enables the Transformer to compute representations of its input and output without using sequential information, making it a powerful model architecture for tasks such as machine translation.\n",
      "[('What makes a transformer different from other neural network?', 'A Transformer is different from other neural networks in that it does not rely on recurrence (such as in recurrent neural networks) or convolution (such as in convolutional neural networks) for capturing dependencies between input and output. Instead, it uses an attention mechanism that allows it to draw global dependencies between input and output. The use of self-attention enables the Transformer to compute representations of its input and output without using sequential information, making it a powerful model architecture for tasks such as machine translation.')]\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "question = 'What makes a transformer different from other neural network?'\n",
    "result, chat_history = ask_with_memory(vector_store=vector_store, q=question, chat_history=chat_history)\n",
    "print(result['answer'])\n",
    "print(chat_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
