{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documents Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load your documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(file):\n",
    "    # Using URL for file param can load data from web.\n",
    "    name, extension = os.path.splitext(file)\n",
    "\n",
    "    if extension == '.pdf':\n",
    "        from langchain.document_loaders import PyPDFLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = PyPDFLoader(file)\n",
    "    elif extension == '.docx':\n",
    "        from langchain.document_loaders import Docx2txtLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = Docx2txtLoader(file)\n",
    "    else:\n",
    "        print('File type not supported!')\n",
    "\n",
    "    data = loader.load()\n",
    "    return data\n",
    "\n",
    "def load_from_wiki(query, lang='en', load_max_docs=1):\n",
    "    from langchain.document_loaders import WikipediaLoader\n",
    "    loader = WikipediaLoader(query=query, lang=lang, load_max_docs=load_max_docs)\n",
    "    data = loader.load\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(data, chunk_size):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding and Uploading to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_or_fetch_embeddings(index_name, chunks=None):\n",
    "    import pinecone\n",
    "    from pinecone import ServerlessSpec\n",
    "    from langchain.vectorstores import Pinecone\n",
    "    from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    pc = pinecone.Pinecone(api_key = os.environ.get('PINECONE_API_KEY'))\n",
    "\n",
    "    indexes = pc.list_indexes()\n",
    "    index_exist = False\n",
    "    for index in indexes:\n",
    "        if index['name'] == index_name:\n",
    "            index_exist = True\n",
    "            vector_store = Pinecone.from_existing_index(index_name, embeddings)\n",
    "    \n",
    "    if not index_exist:\n",
    "        pc.create_index(index_name, dimension=1536, metric='cosine', spec=ServerlessSpec(cloud=\"aws\", region=\"us-west-2\"))\n",
    "        vector_store = Pinecone.from_documents(chunks, embeddings, index_name=index_name)\n",
    "    \n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pinecone_index(index_name='all'):\n",
    "    import pinecone\n",
    "    pc = pinecone.Pinecone(api_key = os.environ.get('PINECONE_API_KEY'))\n",
    "    \n",
    "    if index_name == 'all':\n",
    "        indexes = pc.list_indexes()\n",
    "        for index in indexes:\n",
    "            pc.delete_index(index['name'])\n",
    "    else:\n",
    "        pc.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asking and Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(vector_store, question):\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "    llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=1)\n",
    "\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k':3})\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type='stuff', retriever=retriever)\n",
    "\n",
    "    answer = chain.run(question)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading https://arxiv.org/pdf/1706.03762.pdf\n"
     ]
    }
   ],
   "source": [
    "data = load_document('https://arxiv.org/pdf/1706.03762.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to check the content info\n",
    "# print(f'There is a total of {len(data)} pages.')\n",
    "# print(data[0].page_content)\n",
    "# print(data[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_data(data, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunks[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# Delete all the indexes\n",
    "# delete_pinecone_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "index_name = 'attention'\n",
    "vector_store = insert_or_fetch_embeddings(index_name=index_name, chunks=chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Transformer algorithm is implemented using deep learning frameworks such as TensorFlow or PyTorch. It involves creating a neural network architecture consisting of an encoder and a decoder. The encoder processes the input sequence, while the decoder generates the output sequence. The core components of the Transformer are multi-head self-attention and position-wise fully connected feed-forward networks. These components are stacked together to form multiple layers of the Transformer model. Training the Transformer involves optimizing the model's parameters using techniques like backpropagation and gradient descent.\n"
     ]
    }
   ],
   "source": [
    "question = 'How is the Transformer algorithm impelmented?'\n",
    "answer = get_answer(vector_store, question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type Exit to quit chat.\n",
      "\n",
      "Answer: The Transformer is different from previous models because it relies entirely on self-attention, instead of recurrence, to compute representations of its input and output. It uses stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. This allows the model to draw global dependencies between input and output, resulting in improved performance in tasks such as machine translation.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Answer: There are several ways in which the Transformer model can be improved:\n",
      "\n",
      "1. Model Architecture: Different variations of the Transformer architecture can be explored to improve performance. This can include modifications to the attention mechanism, layer normalization, or the use of additional sublayers.\n",
      "\n",
      "2. Pre-training: Pre-training the Transformer on large amounts of unlabeled data can help improve its performance. This can be done using methods like unsupervised or semi-supervised learning.\n",
      "\n",
      "3. Regularization Techniques: Adding regularization techniques such as dropout or weight decay can help prevent overfitting and improve generalization.\n",
      "\n",
      "4. Larger Training Data: Increasing the size of the training data can improve the model's ability to learn and generalize. This can be done by collecting more data or using data augmentation techniques.\n",
      "\n",
      "5. Parameter Tuning: Careful tuning of hyperparameters such as learning rate, batch size, or the number of layers can improve the model's performance.\n",
      "\n",
      "6. Ensembling: Using an ensemble of multiple Transformer models trained with different initializations or hyperparameters can help improve performance.\n",
      "\n",
      "7. Transfer Learning: Transfer learning can be applied by fine-tuning a pre-trained Transformer model on a specific task, rather than training from scratch. This can help to leverage knowledge learned from a larger dataset or a related task.\n",
      "\n",
      "8. Attention Mechanism Enhancements: Modifications to the attention mechanism, such as incorporating contextual information or incorporating biases, can help improve the Transformer's performance.\n",
      "\n",
      "It's important to note that the effectiveness of these improvements may vary depending on the specific task and dataset being used. Experimentation and empirical evaluation are crucial for determining the best approach to improve the Transformer model.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Answer: I'm sorry, but I'm unable to understand your request. Can you please provide more information or ask a specific question?\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Quitting...\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "i = 1\n",
    "print('Type Exit to quit chat.')\n",
    "while True:\n",
    "    q = input(f'Question {i}: ')\n",
    "    i += 1\n",
    "    \n",
    "    if q.lower() == 'exit':\n",
    "        print('Quitting...')\n",
    "        time.sleep(2)\n",
    "        break\n",
    "\n",
    "    answer = get_answer(vector_store, q)\n",
    "    print(f'\\nAnswer: {answer}')\n",
    "    print(f'\\n{\"-\"* 50}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
