{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.5, max_tokens=1024)\n",
    "messages = [\n",
    "    SystemMessage(content='You are a physicist and respond only in German.'),\n",
    "    HumanMessage(content='Explain quantum mechanics in one sentence.')\n",
    "]\n",
    "output = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantenmechanik beschreibt das Verhalten von Teilchen auf subatomarer Ebene und ermöglicht es uns, ihre Eigenschaften und Interaktionen zu verstehen.\n"
     ]
    }
   ],
   "source": [
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['language', 'virus'] template='You are an experienced virologist.\\nWrite a few sentences about the {virus} in {language}'\n"
     ]
    }
   ],
   "source": [
    "template = '''You are an experienced virologist.\n",
    "Write a few sentences about the {virus} in {language}'''\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = ['virus', 'language'],\n",
    "    template = template\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "埃博拉病毒是一种严重的人畜共患病毒，最早在非洲国家发现。它具有高度传染性和致命性，严重影响了当地社区和社会发展。为了应对这一挑战，国际社会需要密切合作，采取有效的措施来控制和防止疫情的蔓延。作为一名资深的病毒学家，我将竭尽全力研究和应对这一疫情，保障人类健康和社会安全。\n"
     ]
    }
   ],
   "source": [
    "from langchain import OpenAI\n",
    "llm = OpenAI(model_name='gpt-3.5-turbo-instruct', temperature=0.7)\n",
    "output = llm(prompt.format(virus='ebola', language='Chinese'))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.5)\n",
    "template = '''You are an experienced virologist. Write a few sentences about the {virus} in {language}'''\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = ['virus', 'language'],\n",
    "    template = template\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le cancer du poumon est une maladie grave qui se développe généralement à partir des cellules des poumons. Il peut être causé par différents facteurs, tels que le tabagisme, l'exposition à des substances cancérigènes ou des antécédents familiaux. Le cancer du poumon est l'une des principales causes de décès liés au cancer dans le monde. Il existe différents types de cancer du poumon, notamment le carcinome à petites cellules et le carcinome non à petites cellules, chacun nécessitant un traitement spécifique. La prévention, la détection précoce et les traitements innovants sont essentiels pour lutter contre cette maladie mortelle.\n"
     ]
    }
   ],
   "source": [
    "output = chain.run({'virus':'Lung Cancer', 'language':'French'})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms  import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "llm1 = OpenAI(model_name='gpt-3.5-turbo-instruct', temperature=0.7, max_tokens=1024)\n",
    "prompt1 = PromptTemplate(\n",
    "    input_variables=['concept'],\n",
    "    template='''You are an experienced computer scientist and Python programmer. \n",
    "    Write a function that implements the concept of {concept}.'''\n",
    ")\n",
    "chain1 = LLMChain(llm=llm1, prompt=prompt1)\n",
    "\n",
    "llm2 = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=1.2)\n",
    "prompt2 = PromptTemplate(\n",
    "    input_variables=['function'],\n",
    "    template='''Given the python function {function}, describe it as detailed as possible.'''\n",
    ")\n",
    "chain2 = LLMChain(llm=llm2, prompt=prompt2)\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "\n",
      "import threading\n",
      "\n",
      "def multithreading(functions):\n",
      "    # create a thread for each function\n",
      "    threads = [threading.Thread(target=function) for function in functions]\n",
      "    # start each thread\n",
      "    for thread in threads:\n",
      "        thread.start()\n",
      "    # wait for all threads to finish\n",
      "    for thread in threads:\n",
      "        thread.join()\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mThe given Python function `multithreading` is designed to execute multiple functions simultaneously using multiple threads. Here is a detailed description:\n",
      "\n",
      "1. The function imports the `threading` module, which provides classes and methods to manage multiple threads.\n",
      "\n",
      "2. The `multithreading` function accepts a parameter named `functions`, which is expected to be a list of functions that need to be executed concurrently.\n",
      "\n",
      "3. Inside the function, a list called `threads` is initialized to store the thread objects that will be created.\n",
      "\n",
      "4. For each function in the `functions` list, a new thread object is created using the `threading.Thread` class. The `target` argument of each thread is set to the respective function to be executed.\n",
      "\n",
      "5. Once all the thread objects are created, a loop is used to start each thread using the `start()` method. This starts the execution of each function in its own thread, allowing them to run simultaneously.\n",
      "\n",
      "6. After starting all the threads, another loop is used to wait for each thread to finish using the `join()` method. The `join()` method blocks the execution of the main thread until the corresponding thread has finished execution.\n",
      "\n",
      "7. Once all the threads have joined, the `multithreading` function finishes and the program continues with any logic after the function call.\n",
      "\n",
      "By utilizing multiple threads, the `multithreading` function achieves parallel execution of multiple functions. This can be useful in scenarios where different tasks can be performed independently and concurrently, potentially improving the overall performance and response time of the program.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "output =  overall_chain.run('multithreading')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
